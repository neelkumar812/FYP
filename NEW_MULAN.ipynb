{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFIilY1SU_xf"
      },
      "source": [
        "# **Implementation of MuLAN**\n",
        "\n",
        "A joint embedding of music audio and natural language.\n",
        "\n",
        "1.   Implemented and adapted from the paper: https://arxiv.org/abs/2208.12415\n",
        "2.   We are using the MIT AST to analyze music spectrograms, pretrained on AudioSet.\n",
        "3.   The pretrained MuLAN model is used for audio classifcation.\n",
        "4.   For Evaluation, a collection of songs and audio along with its text descriptions are taken from youtube and fed in to the model.\n",
        "5.   The model classifies the test prompts in order of similarity to the audio.\n",
        "6.   This cosine similarity of the prompts are taken with key text captions, and the top k captions are taken as outputs.\n",
        "7.   These outputs are then passed in as input to the SDXL model in order to generate 2D images.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dc9Xm93VYRv"
      },
      "source": [
        "# 1. Import and Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7fs4ugakWxf"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh6SbyUekhGO",
        "outputId": "ac82fa5c-1415-4124-a4e1-48aab581e54b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Ign:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Ign:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Ign:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Ign:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Ign:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Ign:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Ign:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Ign:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Ign:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Ign:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Ign:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Err:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "  503  Service Unavailable [IP: 185.125.190.80 443]\n",
            "Err:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "  503  Service Unavailable [IP: 185.125.190.80 443]\n",
            "Err:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "  503  Service Unavailable [IP: 185.125.190.80 443]\n",
            "Err:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "  503  Service Unavailable [IP: 185.125.190.80 443]\n",
            "Fetched 229 kB in 47s (4,823 B/s)\n",
            "Reading package lists... Done\n",
            "W: Failed to fetch https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu/dists/jammy/InRelease  503  Service Unavailable [IP: 185.125.190.80 443]\n",
            "W: Failed to fetch https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu/dists/jammy/InRelease  503  Service Unavailable [IP: 185.125.190.80 443]\n",
            "W: Failed to fetch https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu/dists/jammy/InRelease  503  Service Unavailable [IP: 185.125.190.80 443]\n",
            "W: Failed to fetch https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu/dists/jammy/InRelease  503  Service Unavailable [IP: 185.125.190.80 443]\n",
            "W: Some index files failed to download. They have been ignored, or old ones used instead.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: gdown==4.6.3 in /usr/local/lib/python3.10/dist-packages (4.6.3)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.3) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.3) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.3) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.3) (4.66.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.3) (4.12.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.10.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.3) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.99)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.6.3) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.3) (1.7.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (7.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install unzip\n",
        "!pip install transformers librosa pandas gdown==4.6.3 torchmetrics # gdown is a tool for downloading files from Google Drive via a URL. provides metrics and evaluation utilities for PyTorch, a popular deep learning framework.\n",
        "!pip install diffusers accelerate --upgrade # This package provides utilities for accelerating deep learning training and inference, often by utilizing hardware acceleration like GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxJDi5upkmU9"
      },
      "source": [
        "## Downloading Dataset and Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K7zpJIHkhJ7",
        "outputId": "6e6413b6-92eb-4bfa-b065-e76781c461d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1llN1-LpZQjyNUy77BWZ98YpQ9Q9nLVoi\n",
            "To: /content/model_MuLan.pt\n",
            "100% 846M/846M [00:12<00:00, 67.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RWqA-0k91j7w90NBciSteVaOnwHrGcY5\n",
            "To: /content/model_timbre.pt\n",
            "100% 346M/346M [00:02<00:00, 131MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1D-qph7MtCexR7jE4wmmHzCOXokgv5TVF\n",
            "To: /content/temp.zip\n",
            "100% 2.14G/2.14G [00:28<00:00, 74.9MB/s]\n",
            "Archive:  temp.zip\n",
            "replace segment/2FQKfGCwjSE.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!gdown 1llN1-LpZQjyNUy77BWZ98YpQ9Q9nLVoi # Installing MuLAN model from https://drive.google.com/uc?id=1llN1-LpZQjyNUy77BWZ98YpQ9Q9nLVoi\n",
        "!gdown 1RWqA-0k91j7w90NBciSteVaOnwHrGcY5 # Installing timre_model from https://drive.google.com/uc?id=1RWqA-0k91j7w90NBciSteVaOnwHrGcY5\n",
        "!gdown 1D-qph7MtCexR7jE4wmmHzCOXokgv5TVF # Installing audio dataset 'temp.zip' from https://drive.google.com/uc?id=1D-qph7MtCexR7jE4wmmHzCOXokgv5TVF\n",
        "!unzip temp.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMRMpteMkyJt"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIvsb1lgj-J4",
        "outputId": "07935a80-0a89-4ed3-bce3-cbf8ca310b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchaudio) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchaudio) (12.4.99)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchaudio) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchaudio) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install torchaudio\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import librosa\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import ASTModel, RobertaModel, AutoTokenizer, AutoFeatureExtractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYm64io5k717"
      },
      "source": [
        "## Hyper-parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BytrMxvIk7lh",
        "outputId": "c92f6f67-af69-4689-be92-a9e1c05cdce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "batch_size = 256 # IMPORTANT: Please reduce the batch size if you have CUDA error out of memory\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(1234)\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHk0LzaFlVQe"
      },
      "source": [
        "# 2. Class Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All three classes (train_dataset, val_dataset, test_dataset) are designed similarly with the following structure:\n",
        "\n",
        "Initialization (init method): When an instance of the class is created, it loads a CSV file containing metadata about the music files. The CSV file path is passed as an argument (csv_file). This CSV likely includes columns for YouTube video IDs (ytid) and associated captions (caption). Each class uses pd.read_csv with different parameters to load specific segments of the dataset: the training class loads the first 4500 rows, the validation class skips the first 4500 rows and loads the next 400 rows, and the testing class skips the first 4900 rows to load the remainder. Additionally, each class initializes a feature extractor model from a pretrained model specified by \"MIT/ast-finetuned-audioset-10-10-0.4593\", which is likely an Audio Spectrogram Transformer (AST) model fine-tuned on the AudioSet dataset for enhanced audio feature extraction.\n",
        "\n",
        "Length (len method): This method returns the total number of items in the dataset (i.e., the number of rows loaded from the CSV file), allowing functions that iterate over the dataset to know when to stop.\n",
        "\n",
        "Get Item (getitem method): This method retrieves a single sample from the dataset at the specified index (idx). It extracts the YouTube video ID and caption for the corresponding row, loads the audio file associated with the YouTube video ID from a specified directory (segment/), resamples the audio to 16,000 Hz using librosa.load, and then passes the waveform to the feature extractor. The feature extractor processes the waveform to produce a spectrogram or some form of processed audio features, which are returned alongside the textual caption. This method enables the dataset to be used with a DataLoader in PyTorch, facilitating batch processing during model training or evaluation."
      ],
      "metadata": {
        "id": "GTEXeZBXfIAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class"
      ],
      "metadata": {
        "id": "6wHNACSCTRMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class train_dataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.music_file = pd.read_csv(csv_file, encoding='utf-8', nrows=4500)\n",
        "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.music_file)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        column = self.music_file.iloc[idx]\n",
        "        ytid = column['ytid']\n",
        "        text = column['caption']\n",
        "        waveform, sample_rate = librosa.load('segment/' + ytid + '.wav', sr = 16000)\n",
        "        spec = self.feature_extractor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
        "        return spec, text\n",
        "\n",
        "class val_dataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.music_file = pd.read_csv(csv_file, encoding='utf-8', skiprows=4500, nrows=400)\n",
        "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.music_file)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        column = self.music_file.iloc[idx]\n",
        "        ytid = column[0]\n",
        "        text = column[5]\n",
        "        waveform, sample_rate = librosa.load('segment/' + ytid + '.wav', sr = 16000)\n",
        "        spec = self.feature_extractor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
        "        return spec, text\n",
        "\n",
        "class test_dataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.music_file = pd.read_csv(csv_file, encoding='utf-8', skiprows=4900)\n",
        "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.music_file)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        column = self.music_file.iloc[idx]\n",
        "        ytid = column[0]\n",
        "        text = column[5]\n",
        "        waveform, sample_rate = librosa.load('segment/' + ytid + '.wav', sr = 16000)\n",
        "        spec = self.feature_extractor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
        "        return spec, text"
      ],
      "metadata": {
        "id": "PomTlfsZHfy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPECIFICS OF EACH CLASS**\n",
        "\n",
        "*train_dataset:* Intended for training the model. It loads the first 4500 rows from the CSV, targeting a larger portion of the data for model training to learn from as many examples as possible.\n",
        "\n",
        "*val_dataset:* Used for validating the model's performance during or after training. It skips the first 4500 rows used for training and loads the next 400 rows, providing a separate dataset that the model has not seen during training to evaluate its generalization capability.\n",
        "\n",
        "*test_dataset:* Used for the final evaluation of the model. It skips the rows used for training and validation, loading the rest of the dataset to test how well the model performs on completely unseen data.\n",
        "\n",
        "Summary These dataset classes are essential components for training, validating, and testing a machine learning model in a structured manner, ensuring that each phase uses distinct data. The use of a feature extractor pre-trained on AudioSet allows for leveraging sophisticated audio representations, potentially improving model performance on tasks requiring"
      ],
      "metadata": {
        "id": "oD07lJUPfQmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traindata = train_dataset('musiccaps.csv')\n",
        "trainLoader = DataLoader(traindata, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "print(traindata.__len__())\n",
        "\n",
        "valdata = val_dataset('musiccaps.csv')\n",
        "valLoader = DataLoader(valdata, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "print(valdata.__len__())\n",
        "\n",
        "testdata = test_dataset('musiccaps.csv')\n",
        "testLoader = DataLoader(testdata, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "print(testdata.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhnQMbfNTe9g",
        "outputId": "aac27d13-45fb-4c93-dc9e-9fa1527d91bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4500\n",
            "400\n",
            "409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the dataset classes previosuly defined for train, validation and test data. Here, the torch utility's dataloader library loads train, val and test data respectively shuffling for each epoch and keeping other technicalities in mind."
      ],
      "metadata": {
        "id": "aThDARqOfheR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEOS0cX1l4cg"
      },
      "source": [
        "## Network Class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class is designed to process audio inputs. It's structured to work with features extracted from audio data, potentially spectrograms or other forms of pre-processed audio signals.\n",
        "\n",
        "Components:\n",
        "\n",
        "ASTModel: Utilizes a pre-trained Audio Spectrogram Transformer (AST) model, specifically \"MIT/ast-finetuned-audioset-10-10-0.4593\". AST models are designed for audio classification tasks and have been fine-tuned on the AudioSet dataset, which contains a wide variety of audio events. This model is likely used to capture rich, high-level representations of audio inputs.\n",
        "\n",
        "audio_fc: A fully connected linear layer that maps the AST model's output (presumably 768 dimensions) to a 128-dimensional space. This reduction in dimensionality may serve to concentrate the audio features into a more compact representation suitable for comparison or fusion with other modalities.\n",
        "\n",
        "Forward Pass:\n",
        "The audio input is first processed by the AST model to obtain a pooled output, which summarizes the audio's features into a single vector.\n",
        "This vector is then passed through the audio_fc linear layer and normalized to ensure that the output vectors lie on a unit hypersphere, which can be particularly useful for similarity comparisons or as part of a joint embedding space with other modalities."
      ],
      "metadata": {
        "id": "qJBSMQOgflEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywDTmKimlzc1"
      },
      "outputs": [],
      "source": [
        "class audio_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(audio_model, self).__init__()\n",
        "        self.model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "        self.audio_fc = nn.Sequential(\n",
        "            nn.Linear(768, 384),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(384, 128)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, audio):\n",
        "        audio_x = self.model(audio).pooler_output\n",
        "        audio_output = self.audio_fc(audio_x)\n",
        "        audio_output = F.normalize(audio_output, p = 2, dim = -1)\n",
        "        return audio_output\n",
        "\n",
        "class text_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(text_model, self).__init__()\n",
        "        self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.text_fc = nn.Sequential(\n",
        "            nn.Linear(768, 384),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(384, 128)\n",
        "        )\n",
        "\n",
        "    def forward(self, text, mask):\n",
        "        text_x = self.model(text, mask).pooler_output\n",
        "        text_output = self.text_fc(text_x)\n",
        "        text_output = F.normalize(text_output, p = 2, dim = -1)\n",
        "        return text_output\n",
        "\n",
        "class mulan(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mulan, self).__init__()\n",
        "        self.audio_model = audio_model()\n",
        "        self.text_model = text_model()\n",
        "\n",
        "    def forward(self, audio, text, mask):\n",
        "        audio_x = self.audio_model(audio)\n",
        "        text_x = self.text_model(text, mask)\n",
        "        return audio_x, text_x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**text_model:**\n",
        "\n",
        " Analogous to audio_model, but designed for processing textual data. Components: RobertaModel: Leverages a pre-trained RoBERTa model (\"roberta-base\"), a powerful transformer-based model known for its effectiveness in natural language processing tasks. This model extracts high-level semantic features from text inputs. text_fc: Similar to audio_fc, this linear layer reduces the dimensionality of the text features from 768 to 128, aiming to produce a compact and informative textual representation. Forward Pass: The text input and its attention mask are passed to the RoBERTa model to obtain a pooled output, encapsulating the text's semantic content in a single vector. This vector undergoes a linear transformation and normalization, similar to the audio model, preparing it for further processing or embedding space integration.\n",
        "\n",
        "**MuLAN Model**\n",
        "\n",
        "Purpose: This class combines audio_model and text_model to work in tandem, facilitating joint processing of audio and text data within a unified model architecture.\n",
        "Components:\n",
        "audio_model and text_model Instances: Embeds instances of both the audio and text models as components of the mulan model, allowing for their simultaneous use.\n",
        "Forward Pass:\n",
        "Accepts audio data, textual data, and a text mask as inputs.\n",
        "Processes the audio data through audio_model and the text data through text_model independently, producing corresponding embeddings.\n",
        "Returns the embeddings from both models, which could then be used for tasks requiring joint audio-text representations, such as cross-modal retrieval, audio-visual alignment, or multimodal classification.\n",
        "This architecture reflects a sophisticated approach to multimodal learning, where the goal is to learn representations that capture the complementary information present in both audio and textual data. By embedding these representations into a shared space, the model can potentially perform a wide range of tasks that require understanding the content and context from both modalities."
      ],
      "metadata": {
        "id": "ApsUI0Oyft6G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJA9giNAmKbD"
      },
      "source": [
        "## Initializing Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmMIN0I-mJkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1368534-07d1-4e03-e1c1-2b33e7557022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "audio_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "mulan_model = mulan()\n",
        "mulan_model = mulan_model.to(device)\n",
        "statedict = torch.load('model_MuLan.pt')\n",
        "for key in list(statedict.keys()):\n",
        "    statedict[key.replace('module.', '')] = statedict.pop(key)\n",
        "mulan_model.load_state_dict(statedict)\n",
        "mulan_model.eval()\n",
        "audio_model = mulan_model.audio_model\n",
        "text_model = mulan_model.text_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Evaluating an example from Music Captions Data"
      ],
      "metadata": {
        "id": "Aq85kpi_G2o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_audio(id, testing_text):\n",
        "  SONG_ID = id\n",
        "  test_data = next(iter(testLoader))\n",
        "  audio, text = test_data\n",
        "  audio = audio['input_values'][SONG_ID].to(device) # use this to be fed into SDXL\n",
        "  text = text[SONG_ID]\n",
        "\n",
        "  test_text = testing_text\n",
        "  tokenized_text = tokenizer(test_text, return_tensors=\"pt\", padding=True)\n",
        "  text_input_ids = tokenized_text['input_ids'].to(device)\n",
        "  text_attention_mask = tokenized_text['attention_mask'].to(device)\n",
        "\n",
        "  audio_model = mulan_model.audio_model\n",
        "  text_model = mulan_model.text_model\n",
        "\n",
        "  audio_embedding = audio_model(audio)\n",
        "  text_embedding = text_model(text_input_ids, text_attention_mask)\n",
        "\n",
        "  sim = torch.einsum('i d, j d -> i j', audio_embedding, text_embedding)\n",
        "  # simply calculate the dot product as similarity\n",
        "  print(sim)\n",
        "  # [-0.2857,  0.8734,  0.3234]\n",
        "  # The result indicates that the 2nd sentence has the highest similarity to the audio input.\n",
        "\n",
        "  return text, audio_embedding, text_embedding"
      ],
      "metadata": {
        "id": "VFBWlwl_G5XS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ID = 34\n",
        "test_text = ['A high quality piano solo', 'A low quality music with vocal singing, guitar, and drum', 'A music of male solo and guitar accompany']\n",
        "\n",
        "text, audio_e, text_e = evaluate_audio(ID, test_text)\n",
        "\n",
        "print(text)\n",
        "print(audio_e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "e1vUWxPuE5BD",
        "outputId": "ac988022-227c-4cb5-b949-6af063eaa9f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'testLoader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-673017998597>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-fab42c1be9ef>\u001b[0m in \u001b[0;36mevaluate_audio\u001b[0;34m(id, testing_text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mSONG_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSONG_ID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use this to be fed into SDXL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'testLoader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An audio is taken from the test dataset. The corresponding text description is \"*The low quality recording features a rock song that contains filtered female vocal singing over loud and harsh electric guitar chords, energetic drums and buzzy bass guitar. It sounds chaotic due to the bad mix and since the drums are panned in an unconventional way - making the stereo image unbalanced. It sounds harsh, but also energetic*\".\n",
        "\n",
        "As a result, we used three test sentences for classification: *'A high quality piano solo', 'A low quality music with vocal singing, guitar, and drum', 'A music of male solo and guitar accompany'*.\n",
        "\n",
        "Ideally, the similariry between the standard answer and the three sentences should be 2 > 3 > 1. And thus we expect the model to predict the similarity following the similarity of the text description."
      ],
      "metadata": {
        "id": "gOab9FkkgJVL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riPTYorgKDol",
        "outputId": "b6459de4-4f0e-4222-b404-574b66bf8dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.2829, -0.4969, -0.2414]], device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVWvPrrPKEKT",
        "outputId": "6ae37fd5-3ede-40bd-bf81-65368bc1faeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0878, -0.0977,  0.0478, -0.1218, -0.0939,  0.2267, -0.0194, -0.1304,\n",
            "          0.2009,  0.0443, -0.0122, -0.0113,  0.0684, -0.1238,  0.0213, -0.0134,\n",
            "          0.1139,  0.0606, -0.0067,  0.0478, -0.1411,  0.0995,  0.1027, -0.0195,\n",
            "          0.0619,  0.0172, -0.0235,  0.1548, -0.0523,  0.1236, -0.1262,  0.0914,\n",
            "         -0.0650,  0.1112,  0.0467, -0.1693,  0.1363,  0.1036, -0.0057,  0.0472,\n",
            "          0.0819, -0.0471,  0.0286,  0.0917, -0.0347, -0.0239,  0.0134,  0.1247,\n",
            "         -0.0858,  0.0046,  0.0751,  0.0217,  0.0199,  0.0389, -0.0525, -0.0146,\n",
            "         -0.0157, -0.0111, -0.1268, -0.0785, -0.0351,  0.0365, -0.0620, -0.0836,\n",
            "         -0.0005, -0.1699,  0.0071,  0.0540,  0.0285,  0.0832, -0.1317,  0.0553,\n",
            "         -0.0566,  0.0812,  0.0746,  0.0289, -0.0703, -0.0202, -0.0042,  0.0910,\n",
            "          0.1415, -0.1476, -0.0143,  0.0369, -0.1501,  0.0715,  0.0777,  0.0474,\n",
            "         -0.0256,  0.1452, -0.1847,  0.0464,  0.1203, -0.0784, -0.0987, -0.1293,\n",
            "          0.0045,  0.0451, -0.1366,  0.0181,  0.0585,  0.0038,  0.2504,  0.0873,\n",
            "         -0.0242, -0.0216,  0.1144, -0.0081, -0.0075,  0.0572, -0.0730, -0.0037,\n",
            "         -0.1728, -0.0398,  0.1109,  0.1099, -0.0347,  0.0291, -0.0304, -0.0823,\n",
            "         -0.1114,  0.0375,  0.0692, -0.0760, -0.0433, -0.1496, -0.0053,  0.1379]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "torch.Size([1, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Calculating Cosine Matrix and Output Keys\n"
      ],
      "metadata": {
        "id": "-L4bdMP8cPN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### !! No need to run **file below** after uploading text_keys to google drive once"
      ],
      "metadata": {
        "id": "_5Rls_BNcXk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload a file\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "HbmFCjDJKc7n",
        "outputId": "7bc66f30-1e23-438b-bf29-706bd2fdac35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a1b66f0-efd5-49f4-84b0-35329e5e930e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a1b66f0-efd5-49f4-84b0-35329e5e930e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving text_keys.txt to text_keys.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file name\n",
        "file_name = 'text_keys.txt'  # Replace with the actual uploaded file name\n",
        "\n",
        "with open(file_name, 'r') as file:\n",
        "    content = file.readlines()\n",
        "\n",
        "# Convert to Python List\n",
        "tkeys = [line.strip() for line in content]\n",
        "print(tkeys)\n",
        "print(len(tkeys))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoiG20SuKhpD",
        "outputId": "0f878e84-71f8-4364-b80e-9a6cf1e58b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exciting', 'vibrant', 'joyful', 'playful', 'buoyant', 'animated', 'uplifting', 'inspiring', 'passionate', 'energetic', 'groovy', 'aggressive', 'funky', 'epic', 'romantic', 'sentimental', 'melancholic', 'soulful', 'nostalgic', 'heartfelt', 'relaxing', 'melodic', 'pleasant', 'chill', 'dreamy', 'meditative', 'engaging', 'suspenseful', 'dramatic', 'distorted', 'eerie', 'rock genre', 'electronic genre', 'ambient genre', 'hip hop genre', 'pop song', 'film music genre', 'jazz genre', 'folk music genre', 'love song', 'classical genre', 'country genre', 'upbeat tempo', 'driving tempo', 'leisurely tempo', 'measured tempo', 'languid tempo', 'flexible tempo', 'rubato', 'loud', 'soft', 'lively', 'soothing', 'intense', 'sudden dynamic shifts', 'gradual build up', 'steady dynamics', 'big range of dynamics', 'smooth dynamic contours', 'steady rhythm', 'syncopated rhythm', 'offbeat rhythm', 'swinging rhythm', 'irregular rhythm', 'regular rhythm', 'pulsating rhythm', 'repeated theme', 'verse and chorus structure', 'binary structure', 'ternary structure', 'theme and variations', 'complex structure', 'linear structure', 'cyclical structure', 'piano', 'percussion', 'bass', 'strings', 'orchestra', 'brass', 'multiple instruments', 'single instrument', 'simple harmony', 'complex harmony', 'polyphonic texture', 'homophonic texture', 'sparse texture', 'dense texture', 'arpeggiated accompaniment', 'chordal accompaniment', 'staccato', 'legato', 'grand texture', 'dissonant harmony', 'reverb', 'mellow tone', 'harsh tone', 'warm sound', 'cold sound', 'dark music', 'clean music', 'sustained melody', 'catchy melody', 'percussive bass line', 'flowing melody', 'haunting melody', 'contemplative melody', 'walking bass line', 'sustained bass line', 'rapid succession of notes', 'dramatic pause', 'extended musical lines', 'brief musical segments']\n",
            "113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tkeys = tokenizer(tkeys, return_tensors=\"pt\", padding=True)\n",
        "tkeys_input_ids = tokenized_tkeys['input_ids'].to(device)\n",
        "tkeys_attention_mask = tokenized_tkeys['attention_mask'].to(device)\n"
      ],
      "metadata": {
        "id": "MX1QXrrIKkxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tkeys_embedding = text_model(tkeys_input_ids, tkeys_attention_mask)"
      ],
      "metadata": {
        "id": "S8HpEQDEKmuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_matrix = torch.einsum('i d, j d -> i j', audio_embedding, tkeys_embedding) # simply calculate the dot product as similarity\n",
        "print(cosine_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng3naEPlKoUd",
        "outputId": "b1ce93cf-c109-4c69-a9b3-d166941ed6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1842,  0.6502, -0.3024,  0.1244, -0.0248, -0.0451,  0.0048, -0.1150,\n",
            "         -0.2462,  0.1571, -0.1453,  0.4833,  0.6967, -0.1221, -0.3116, -0.0669,\n",
            "         -0.3280, -0.2760, -0.2402, -0.3712, -0.2423, -0.2658, -0.1754, -0.2941,\n",
            "         -0.2604, -0.1357, -0.0757, -0.1811,  0.0610, -0.0051, -0.3115, -0.0687,\n",
            "          0.0769, -0.1950,  0.9505,  0.0572, -0.2787, -0.2318, -0.3539, -0.3205,\n",
            "         -0.3097, -0.3250,  0.2932,  0.1228, -0.1469, -0.2403, -0.1662,  0.1539,\n",
            "          0.0196, -0.1007, -0.2920, -0.3261, -0.1344, -0.2387, -0.0369, -0.0238,\n",
            "         -0.1510, -0.0449,  0.1894,  0.2761,  0.3031,  0.7652,  0.7055,  0.2085,\n",
            "          0.4293,  0.2069, -0.0849, -0.3158, -0.0409, -0.2272, -0.2774, -0.2071,\n",
            "         -0.2093, -0.1954, -0.3086,  0.5207,  0.0570, -0.3248, -0.2800,  0.1158,\n",
            "          0.0267, -0.2796, -0.3213, -0.3097, -0.1559, -0.2367, -0.2125, -0.2366,\n",
            "         -0.2750, -0.2999, -0.0554, -0.2212, -0.2627, -0.2983, -0.1915, -0.2854,\n",
            "         -0.1684, -0.1305, -0.0898, -0.1620,  0.2189, -0.3329, -0.2332,  0.5353,\n",
            "         -0.3470, -0.2682, -0.3381,  0.1401,  0.1728, -0.2617, -0.2137, -0.3560,\n",
            "         -0.3278]], device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the cosine matrix and get the indices of the top 10 values\n",
        "flat_cosine_matrix = cosine_matrix.view(-1)\n",
        "top_values, top_indices = torch.topk(flat_cosine_matrix, k=10)\n",
        "\n",
        "# Initialize lists to store the top 7 values and their indices\n",
        "top_values_list = []\n",
        "top_indices_list = []\n",
        "\n",
        "# Append top values and their indices to the respective lists\n",
        "for i in range(len(top_values)):\n",
        "    top_values_list.append(top_values[i].item())\n",
        "    row_index = top_indices[i] // cosine_matrix.size(1)\n",
        "    col_index = top_indices[i] % cosine_matrix.size(1)\n",
        "    top_indices_list.append((row_index.item(), col_index.item()))\n",
        "\n",
        "print(\"Top 10 values:\", top_values_list)\n",
        "print(\"Indices of the top 10 values:\", top_indices_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHlTea0lKq0p",
        "outputId": "639f8426-9ebd-410d-fe32-66fa4f9d2044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 values: [0.9504696130752563, 0.765236496925354, 0.7055051922798157, 0.6966748237609863, 0.6502219438552856, 0.535324215888977, 0.520749568939209, 0.4832949638366699, 0.4293314814567566, 0.30305320024490356]\n",
            "Indices of the top 10 values: [(0, 34), (0, 61), (0, 62), (0, 12), (0, 1), (0, 103), (0, 75), (0, 11), (0, 64), (0, 60)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new list to store the extracted items\n",
        "extracted_items = []\n",
        "\n",
        "# Extract items from newlist based on the indices and add them to extracted_items\n",
        "for index_pair in top_indices_list:\n",
        "    row_index, col_index = index_pair\n",
        "    index = row_index * len(tkeys[0]) + col_index\n",
        "    extracted_items.append(tkeys[index])\n",
        "\n",
        "output = ', '.join(extracted_items)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIY-HCwyKt_Z",
        "outputId": "8be42f15-9b32-4f4a-9770-ea4a8055f94c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hip hop genre, offbeat rhythm, swinging rhythm, funky, vibrant, percussive bass line, percussion, aggressive, regular rhythm, syncopated rhythm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity_tensor = torch.tensor(cosine_matrix, device='cuda:0')\n",
        "\n",
        "# Convert the tensor to a NumPy array\n",
        "cosine_similarity_array = cosine_similarity_tensor.cpu().detach().numpy()\n",
        "\n",
        "# Analyze the distribution of scores\n",
        "mean_score = cosine_similarity_array.mean()\n",
        "std_dev = cosine_similarity_array.std()\n",
        "\n",
        "# Set the threshold based on mean and standard deviation\n",
        "threshold = mean_score + 2.3 * std_dev  # You can adjust the multiplier as needed\n",
        "\n",
        "# Filter text descriptions based on the threshold\n",
        "good_descriptions = [score for score in cosine_similarity_array[0] if score >= threshold]\n",
        "inaccurate_descriptions = [score for score in cosine_similarity_array[0] if score < threshold]\n",
        "\n",
        "print(\"Threshold:\", threshold)\n",
        "print(\"Good Descriptions:\", good_descriptions)\n",
        "print(\"Inaccurate Descriptions:\", inaccurate_descriptions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5j5fjThKzU0",
        "outputId": "ba5daa5f-e10e-46ec-88b5-92c3a7b10932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.055050058662891366\n",
            "Good Descriptions: [0.28316426, 0.12839445]\n",
            "Inaccurate Descriptions: [-0.15461688, -0.33816493, -0.25643817, -0.36298656, -0.21067788, -0.22419176, -0.10566264, -0.23600757, -0.32691246, -0.22349346, -0.31646547, -0.42285937, -0.32757074, -0.1149856, -0.21383023, -0.17365645, -0.4613264, 0.008459777, -0.14053553, -0.26598924, -0.1375659, -0.1872864, -0.26141763, -0.20761073, -0.14513937, -0.24123429, -0.3839349, -0.11058312, -0.25161153, -0.3162837, -0.2432096, -0.23976976, -0.2025727, -0.38178623, -0.082676575, -0.25843358, -0.25570387, -0.49184602, -0.2539662, -0.33682257, -0.3540408, -0.32770595, -0.21802464, -0.3348496, -0.28769612, -0.26976502, -0.33389604, -0.5053242, -0.38588446, -0.20909336, -0.20754328, -0.24904914, -0.23116156, -0.11657621, -0.19682577, -0.14867371, -0.1683981, -0.25464207, -0.27564687, -0.23862515, -0.3594663, -0.4109816, -0.3046247, -0.32258853, -0.18550923, -0.24982587, -0.4474535, -0.21344309, -0.18944913, -0.2862116, -0.17460194, -0.13993992, -0.20905375, -0.41916525, -0.41314855, -0.18484859, -0.23012818, -0.3297197, -0.021304082, -0.33284885, -0.39610782, -0.18022808, -0.16368818, -0.07851029, -0.14433661, -0.111344725, -0.105019644, -0.40402606, -0.39650777, -0.43065646, -0.4553594, -0.17857389, -0.19150174, -0.3144501, -0.12483771, -0.32185206, -0.056774423, -0.06287074, -0.06454165, -0.19288075, -0.30989015, -0.2041192, 0.02865123, -0.21203853, -0.13546881, -0.25664917, 0.00091134757, -0.1776723, -0.17679523, -0.3519625, -0.36993414]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-bbf317b8b3c2>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  cosine_similarity_tensor = torch.tensor(cosine_matrix, device='cuda:0')\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}